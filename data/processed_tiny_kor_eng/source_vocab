<pad>
<unk>
<s>
<\s>
.
이
있습니다
'
Attention
그리고
쿼리
수
,
입니다
을
를
및
키
값
여기서
출력은
모델은
좋은
안녕하세요
반갑습니다
딥러닝은
정말
흥미로운
분야에요
그래서
저는
모델을
구현하면서
공부하고
논문의
제목은
Is
All
You
Need
저자는
기존의
Seq
2Seq
에서
사용되던
RNN
이나
CNN
사용하지
않았습니다
대신에
FeedForward
와
으로
구성되어
문장의
위치를
입력값에
넣기
위해서
sinusodial
사용합니다
핵심이
되는
은
아래와
같이
정리가
됩니다
주의력
함수는
쌍
집합을
출력에
매핑
하는
것으로
설명할
모두
벡터입니다
각
값에
할당된
가중치
합계가
해당
키와의
호환성
기능에
의해
계산되는
값의
가중
합계로
계산됩니다
헤드의
숫자를
늘리면
속도와
성능이
둘다
올라갑니다
multi-head
attention
통해
다양한
위치에서
서로
다른
표현
하위
영역의
정보에
공동으로
관심을
기울일
병렬화를
수행할
있으므로
교육
시간이
크게
단축됩니다
구글
리서치
블로그에서는
공동
참조
해상도'에
대해
성능
발휘
한다고
소개합니다
코드들이
당신에게
도움이
되었으면
좋겠습니다
저장소에
Star
주시는
것을
잊지마세요
모델에
문제가
있거나
개선할
부분이
있으면
언제든지
pull
request
날려주세요
한글에서
영어로
번역되는
작은
데이터셋은
여기까지
데이터셋을
읽어주셔서
감사합니다
하루
보내시길
바랍니다
